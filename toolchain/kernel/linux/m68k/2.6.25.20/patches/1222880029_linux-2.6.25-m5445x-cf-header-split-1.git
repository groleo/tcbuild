From a9447bc5393a1c9c1c964d265da8c02787a1f8a7 Mon Sep 17 00:00:00 2001
From: Kurt Mahan <kmahan@freescale.com>
Date: Wed, 1 Oct 2008 10:53:49 -0600
Subject: [PATCH] Separate coldfire headers from m68k headers.

Creating coldfire specific header files to reduce the ifdef
headaches that have been created by attempting to merge
coldfire and m68k headers.

LTIBName: m5445x-cf-header-split-1
Signed-off-by: Kurt Mahan <kmahan@freescale.com>
---
 include/asm-m68k/bitops.h         |  502 +------------------------------------
 include/asm-m68k/cf_bitops.h      |  438 ++++++++++++++++++++++++++++++++
 include/asm-m68k/cf_io.h          |  175 +++++++++++++
 include/asm-m68k/cf_raw_io.h      |  167 ++++++++++++
 include/asm-m68k/cf_virtconvert.h |   55 ++++
 include/asm-m68k/io.h             |   83 +------
 include/asm-m68k/raw_io.h         |   93 +-------
 include/asm-m68k/virtconvert.h    |   15 +-
 8 files changed, 855 insertions(+), 673 deletions(-)
 create mode 100644 include/asm-m68k/cf_bitops.h
 create mode 100644 include/asm-m68k/cf_io.h
 create mode 100644 include/asm-m68k/cf_raw_io.h
 create mode 100644 include/asm-m68k/cf_virtconvert.h

diff --git a/include/asm-m68k/bitops.h b/include/asm-m68k/bitops.h
index ad62ad1..522c7df 100644
--- a/include/asm-m68k/bitops.h
+++ b/include/asm-m68k/bitops.h
@@ -8,6 +8,10 @@
  * for more details.
  */
 
+#ifdef CONFIG_COLDFIRE
+#include <asm/cf_bitops.h>
+#else
+
 #ifndef _LINUX_BITOPS_H
 #error only <linux/bitops.h> can be included directly
 #endif
@@ -19,7 +23,6 @@
  *
  * They use the standard big-endian m680x0 bit ordering.
  */
-#ifndef CONFIG_COLDFIRE
 
 #define test_and_set_bit(nr,vaddr) \
   (__builtin_constant_p(nr) ? \
@@ -416,503 +419,6 @@ static inline int ext2_find_next_zero_bit(const void *vaddr, unsigned size,
 
 #endif /* __KERNEL__ */
 
-#else /* CONFIG_COLDFIRE */
-
-#define test_and_set_bit(nr,vaddr)			\
-  (__builtin_constant_p(nr) ?				\
-   __constant_coldfire_test_and_set_bit(nr, vaddr) :	\
-   __generic_coldfire_test_and_set_bit(nr, vaddr))
-
-#if 0
-static __inline__ int __constant_coldfire_test_and_set_bit(int nr,
-	volatile void *vaddr)
-{
-	char retval;
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-
-	__asm__ __volatile__ ("bset %2,%1; sne %0"
-	     : "=d" (retval), "+QUd" (*p)
-	     : "di" (nr & 7));
-	return retval;
-}
-#else
-static __inline__ int __constant_coldfire_test_and_set_bit(int nr,
-	volatile void *vaddr)
-{
-	char retval;
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-	__asm__ __volatile__ ("bset %2,(%4); sne %0"
-	     : "=d" (retval), "=m" (*p)
-	     : "di" (nr & 7), "m" (*p), "a" (p));
-	return retval;
-}
-#endif
-
-static __inline__ int __generic_coldfire_test_and_set_bit(int nr,
-	volatile void *vaddr)
-{
-	char retval;
-
-	__asm__ __volatile__ ("bset %2,%1; sne %0"
-	     : "=d" (retval), "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
-	     : "d" (nr)
-	     : "memory");
-	return retval;
-}
-#define __test_and_set_bit(nr, vaddr) test_and_set_bit(nr, vaddr)
-
-#define set_bit(nr,vaddr)			\
-  (__builtin_constant_p(nr) ?			\
-   __constant_coldfire_set_bit(nr, vaddr) :	\
-   __generic_coldfire_set_bit(nr, vaddr))
-
-#if 0
-static __inline__ void __constant_coldfire_set_bit(int nr,
-	volatile void *vaddr)
-{
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-	__asm__ __volatile__ ("bset %1,%0"
-	     : "+QUd" (*p) : "di" (nr & 7));
-}
-#else
-static __inline__ void __constant_coldfire_set_bit(int nr,
-	volatile void *vaddr)
-{
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-	__asm__ __volatile__ ("bset %1,(%3)"
-	     : "=m" (*p) : "di" (nr & 7), "m" (*p), "a" (p));
-}
-#endif
-
-static __inline__ void __generic_coldfire_set_bit(int nr,
-	volatile void *vaddr)
-{
-	__asm__ __volatile__ ("bset %1,%0"
-	     : "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
-	     : "d" (nr)
-	     : "memory");
-}
-#define __set_bit(nr, vaddr) set_bit(nr, vaddr)
-
-#define test_and_clear_bit(nr, vaddr)			\
-  (__builtin_constant_p(nr) ?				\
-   __constant_coldfire_test_and_clear_bit(nr, vaddr) :	\
-   __generic_coldfire_test_and_clear_bit(nr, vaddr))
-
-#if 0
-static __inline__ int __constant_coldfire_test_and_clear_bit(int nr,
-	volatile void *vaddr)
-{
-	char retval;
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-
-	__asm__ __volatile__ ("bclr %2,%1; sne %0"
-	     : "=d" (retval), "+QUd" (*p)
-	     : "id" (nr & 7));
-
-	return retval;
-}
-#else
-static __inline__ int __constant_coldfire_test_and_clear_bit(int nr,
-	volatile void *vaddr)
-{
-	char retval;
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-
-	__asm__ __volatile__ ("bclr %2,(%4); sne %0"
-	     : "=d" (retval), "=m" (*p)
-	     : "id" (nr & 7), "m" (*p), "a" (p));
-
-	return retval;
-}
-#endif
-
-static __inline__ int __generic_coldfire_test_and_clear_bit(int nr,
-	volatile void *vaddr)
-{
-	char retval;
-
-	__asm__ __volatile__ ("bclr %2,%1; sne %0"
-	     : "=d" (retval), "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
-	     : "d" (nr & 7)
-	     : "memory");
-
-	return retval;
-}
-#define __test_and_clear_bit(nr, vaddr) test_and_clear_bit(nr, vaddr)
-
-/*
- * clear_bit() doesn't provide any barrier for the compiler.
- */
-#define smp_mb__before_clear_bit()	barrier()
-#define smp_mb__after_clear_bit()	barrier()
-
-#define clear_bit(nr,vaddr)			\
-  (__builtin_constant_p(nr) ?			\
-   __constant_coldfire_clear_bit(nr, vaddr) :	\
-   __generic_coldfire_clear_bit(nr, vaddr))
-
-#if 0
-static __inline__ void __constant_coldfire_clear_bit(int nr,
-	volatile void *vaddr)
-{
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-	__asm__ __volatile__ ("bclr %1,%0"
-	     : "+QUd" (*p) : "id" (nr & 7));
-}
-#else
-static __inline__ void __constant_coldfire_clear_bit(int nr,
-	volatile void *vaddr)
-{
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-	__asm__ __volatile__ ("bclr %1,(%3)"
-	     : "=m" (*p) : "id" (nr & 7), "m" (*p), "a" (p));
-}
-#endif
-
-static __inline__ void __generic_coldfire_clear_bit(int nr,
-	volatile void *vaddr)
-{
-	__asm__ __volatile__ ("bclr %1,%0"
-	     : "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
-	     : "d" (nr)
-	     : "memory");
-}
-#define __clear_bit(nr, vaddr) clear_bit(nr, vaddr)
-
-#define test_and_change_bit(nr, vaddr)			\
-  (__builtin_constant_p(nr) ?				\
-   __constant_coldfire_test_and_change_bit(nr, vaddr) :	\
-   __generic_coldfire_test_and_change_bit(nr, vaddr))
-
-#if 0
-static __inline__ int __constant_coldfire_test_and_change_bit(int nr,
-	volatile void *vaddr)
-{
-	char retval;
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-
-	__asm__ __volatile__ ("bchg %2,%1; sne %0"
-	     : "=d" (retval), "+QUd" (*p)
-	     : "id" (nr & 7));
-
-	return retval;
-}
-#else
-static __inline__ int __constant_coldfire_test_and_change_bit(int nr,
-	volatile void *vaddr)
-{
-	char retval;
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-
-	__asm__ __volatile__ ("bchg %2,(%4); sne %0"
-	     : "=d" (retval), "=m" (*p)
-	     : "id" (nr & 7), "m" (*p), "a" (p));
-
-	return retval;
-}
-#endif
-
-static __inline__ int __generic_coldfire_test_and_change_bit(int nr,
-	volatile void *vaddr)
-{
-	char retval;
-
-	__asm__ __volatile__ ("bchg %2,%1; sne %0"
-	     : "=d" (retval), "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
-	     : "id" (nr)
-	     : "memory");
-
-	return retval;
-}
-#define __test_and_change_bit(nr, vaddr) test_and_change_bit(nr, vaddr)
-#define __change_bit(nr, vaddr) change_bit(nr, vaddr)
-
-#define change_bit(nr,vaddr)			\
-  (__builtin_constant_p(nr) ?			\
-   __constant_coldfire_change_bit(nr, vaddr) :	\
-   __generic_coldfire_change_bit(nr, vaddr))
-
-#if 0
-static __inline__ void __constant_coldfire_change_bit(int nr,
-	volatile void *vaddr)
-{
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-	__asm__ __volatile__ ("bchg %1,%0"
-	     : "+QUd" (*p) : "id" (nr & 7));
-}
-#else
-static __inline__ void __constant_coldfire_change_bit(int nr,
-	volatile void *vaddr)
-{
-	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
-	__asm__ __volatile__ ("bchg %1,(%3)"
-	     : "=m" (*p) : "id" (nr & 7), "m" (*p), "a" (p));
-}
-#endif
-
-static __inline__ void __generic_coldfire_change_bit(int nr,
-	volatile void *vaddr)
-{
-	__asm__ __volatile__ ("bchg %1,%0"
-	     : "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
-	     : "d" (nr)
-	     : "memory");
-}
-
-static inline int test_bit(int nr, const unsigned long *vaddr)
-{
-	return (vaddr[nr >> 5] & (1UL << (nr & 31))) != 0;
-}
-
-static __inline__ unsigned long ffz(unsigned long word)
-{
-	unsigned long result = 0;
-
-	while (word & 1) {
-		result++;
-		word >>= 1;
-	}
-	return result;
-}
-
-/* find_next_zero_bit() finds the first zero bit in a bit string of length
- * 'size' bits, starting the search at bit 'offset'.  This is largely based
- * on Linus's ALPHA routines.
- */
-static __inline__ unsigned long find_next_zero_bit(void *addr,
-	unsigned long size, unsigned long offset)
-{
-	unsigned long *p = ((unsigned long *) addr) + (offset >> 5);
-	unsigned long result = offset & ~31UL;
-	unsigned long tmp;
-
-	if (offset >= size)
-		return size;
-	size -= result;
-	offset &= 31UL;
-	if (offset) {
-		tmp = *(p++);
-		tmp |= ~0UL >> (32-offset);
-		if (size < 32)
-			goto found_first;
-		if (~tmp)
-			goto found_middle;
-		size -= 32;
-		result += 32;
-	}
-	while (size & ~31UL) {
-		tmp = *(p++);
-		if (~tmp)
-			goto found_middle;
-		result += 32;
-		size -= 32;
-	}
-	if (!size)
-		return result;
-	tmp = *p;
-
-found_first:
-	tmp |= ~0UL >> size;
-found_middle:
-	return result + ffz(tmp);
-}
-
-#define find_first_zero_bit(addr, size) find_next_zero_bit(((void *)addr), \
-		(size), 0)
-
-/* Ported from included/linux/bitops.h  */
-static __inline__ int ffs(int x)
-{
-	int r = 1;
-
-	if (!x)
-		return 0;
-	if (!(x & 0xffff)) {
-		x >>= 16;
-		r += 16;
-	}
-	if (!(x & 0xff)) {
-		x >>= 8;
-		r += 8;
-	}
-	if (!(x & 0xf)) {
-		x >>= 4;
-		r += 4;
-	}
-	if (!(x & 3)) {
-		x >>= 2;
-		r += 2;
-	}
-	if (!(x & 1)) {
-		x >>= 1;
-		r += 1;
-	}
-	return r;
-}
-#define __ffs(x) (ffs(x) - 1)
-
-/* find_next_bit - find the next set bit in a memory region
- * (from asm-ppc/bitops.h)
- */
-static __inline__ unsigned long find_next_bit(const unsigned long *addr,
-	unsigned long size, unsigned long offset)
-{
-	unsigned int *p = ((unsigned int *) addr) + (offset >> 5);
-	unsigned int result = offset & ~31UL;
-	unsigned int tmp;
-
-	if (offset >= size)
-		return size;
-	size -= result;
-	offset &= 31UL;
-	if (offset) {
-		tmp = *p++;
-		tmp &= ~0UL << offset;
-		if (size < 32)
-			goto found_first;
-		if (tmp)
-			goto found_middle;
-		size -= 32;
-		result += 32;
-	}
-	while (size >= 32) {
-		tmp = *p++;
-		if (tmp != 0)
-			goto found_middle;
-		result += 32;
-		size -= 32;
-	}
-	if (!size)
-		return result;
-	tmp = *p;
-
-found_first:
-	tmp &= ~0UL >> (32 - size);
-	if (tmp == 0UL)        /* Are any bits set? */
-		return result + size; /* Nope. */
-found_middle:
-	return result + __ffs(tmp);
-}
-
-#define find_first_bit(addr, size) find_next_bit((addr), (size), 0)
-
-#ifdef __KERNEL__
-
-/* Ported from include/linux/bitops.h */
-static  __inline__ int fls(int x)
-{
-	int r = 32;
-
-	if (!x)
-		return 0;
-	if (!(x & 0xffff0000u)) {
-		x <<= 16;
-		r -= 16;
-	}
-	if (!(x & 0xff000000u)) {
-		x <<= 8;
-		r -= 8;
-	}
-	if (!(x & 0xf0000000u)) {
-		x <<= 4;
-		r -= 4;
-	}
-	if (!(x & 0xc0000000u)) {
-		x <<= 2;
-		r -= 2;
-	}
-	if (!(x & 0x80000000u)) {
-		x <<= 1;
-		r -= 1;
-	}
-	return r;
-}
-
-#include <asm-generic/bitops/fls64.h>
-#include <asm-generic/bitops/sched.h>
-#include <asm-generic/bitops/hweight.h>
-#include <asm-generic/bitops/lock.h>
-
-#define minix_find_first_zero_bit(addr, size)	find_next_zero_bit((addr), \
-							(size), 0)
-#define minix_test_and_set_bit(nr, addr)	test_and_set_bit((nr), \
-							(unsigned long *)(addr))
-#define minix_set_bit(nr, addr)			set_bit((nr), \
-							(unsigned long *)(addr))
-#define minix_test_and_clear_bit(nr, addr)	test_and_clear_bit((nr), \
-							(unsigned long *)(addr))
-
-static inline int minix_test_bit(int nr, const volatile unsigned long *vaddr)
-{
-	int 	*a = (int *)vaddr;
-	int	mask;
-
-	a += nr >> 5;
-	mask = 1 << (nr & 0x1f);
-	return ((mask & *a) != 0);
-}
-
-#define ext2_set_bit(nr, addr)			test_and_set_bit((nr) ^ 24, \
-							(unsigned long *)(addr))
-#define ext2_set_bit_atomic(lock, nr, addr)	test_and_set_bit((nr) ^ 24, \
-							(unsigned long *)(addr))
-#define ext2_clear_bit(nr, addr)		test_and_clear_bit((nr) ^ 24, \
-							(unsigned long *)(addr))
-#define ext2_clear_bit_atomic(lock, nr, addr)	test_and_clear_bit((nr) ^ 24, \
-							(unsigned long *)(addr))
-
-static inline int ext2_test_bit(int nr, const void *vaddr)
-{
-	const unsigned char *p = vaddr;
-	return (p[nr >> 3] & (1U << (nr & 7))) != 0;
-}
-
-static inline int ext2_find_first_zero_bit(const void *vaddr, unsigned size)
-{
-	const unsigned long *p = vaddr, *addr = vaddr;
-	int res;
-
-	if (!size)
-		return 0;
-
-	size = (size >> 5) + ((size & 31) > 0);
-	while (*p++ == ~0UL) {
-		if (--size == 0)
-			return (p - addr) << 5;
-	}
-
-	--p;
-	for (res = 0; res < 32; res++)
-		if (!ext2_test_bit (res, p))
-			break;
-	return (p - addr) * 32 + res;
-}
-
-static inline int ext2_find_next_zero_bit(const void *vaddr, unsigned size,
-					  unsigned offset)
-{
-	const unsigned long *addr = vaddr;
-	const unsigned long *p = addr + (offset >> 5);
-	int bit = offset & 31UL, res;
-
-	if (offset >= size)
-		return size;
-
-	if (bit) {
-		/* Look for zero in first longword */
-		for (res = bit; res < 32; res++)
-			if (!ext2_test_bit (res, p))
-				return (p - addr) * 32 + res;
-		p++;
-	}
-	/* No zero yet, search remaining full bytes for a zero */
-	res = ext2_find_first_zero_bit(p, size - 32 * (p - addr));
-	return (p - addr) * 32 + res;
-}
-
-#endif /* KERNEL */
-
 #endif /* CONFIG_COLDFIRE */
 
 #endif /* _M68K_BITOPS_H */
diff --git a/include/asm-m68k/cf_bitops.h b/include/asm-m68k/cf_bitops.h
new file mode 100644
index 0000000..1f6b2fa
--- /dev/null
+++ b/include/asm-m68k/cf_bitops.h
@@ -0,0 +1,438 @@
+#ifndef __CF_BITOPS__
+#define __CF_BITOPS__
+/*
+ * Copyright 1992, Linus Torvalds.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file COPYING in the main directory of this archive
+ * for more details.
+ */
+
+#ifndef _LINUX_BITOPS_H
+#error only <linux/bitops.h> can be included directly
+#endif
+
+#include <linux/compiler.h>
+
+#define test_and_set_bit(nr,vaddr)			\
+  (__builtin_constant_p(nr) ?				\
+   __constant_coldfire_test_and_set_bit(nr, vaddr) :	\
+   __generic_coldfire_test_and_set_bit(nr, vaddr))
+
+static __inline__ int __constant_coldfire_test_and_set_bit(int nr,
+	volatile void *vaddr)
+{
+	char retval;
+	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
+	__asm__ __volatile__ ("bset %2,(%4); sne %0"
+	     : "=d" (retval), "=m" (*p)
+	     : "di" (nr & 7), "m" (*p), "a" (p));
+	return retval;
+}
+
+static __inline__ int __generic_coldfire_test_and_set_bit(int nr,
+	volatile void *vaddr)
+{
+	char retval;
+
+	__asm__ __volatile__ ("bset %2,%1; sne %0"
+	     : "=d" (retval), "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
+	     : "d" (nr)
+	     : "memory");
+	return retval;
+}
+#define __test_and_set_bit(nr, vaddr) test_and_set_bit(nr, vaddr)
+
+#define set_bit(nr,vaddr)			\
+  (__builtin_constant_p(nr) ?			\
+   __constant_coldfire_set_bit(nr, vaddr) :	\
+   __generic_coldfire_set_bit(nr, vaddr))
+
+static __inline__ void __constant_coldfire_set_bit(int nr,
+	volatile void *vaddr)
+{
+	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
+	__asm__ __volatile__ ("bset %1,(%3)"
+	     : "=m" (*p) : "di" (nr & 7), "m" (*p), "a" (p));
+}
+
+static __inline__ void __generic_coldfire_set_bit(int nr,
+	volatile void *vaddr)
+{
+	__asm__ __volatile__ ("bset %1,%0"
+	     : "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
+	     : "d" (nr)
+	     : "memory");
+}
+#define __set_bit(nr, vaddr) set_bit(nr, vaddr)
+
+#define test_and_clear_bit(nr, vaddr)			\
+  (__builtin_constant_p(nr) ?				\
+   __constant_coldfire_test_and_clear_bit(nr, vaddr) :	\
+   __generic_coldfire_test_and_clear_bit(nr, vaddr))
+
+static __inline__ int __constant_coldfire_test_and_clear_bit(int nr,
+	volatile void *vaddr)
+{
+	char retval;
+	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
+
+	__asm__ __volatile__ ("bclr %2,(%4); sne %0"
+	     : "=d" (retval), "=m" (*p)
+	     : "id" (nr & 7), "m" (*p), "a" (p));
+
+	return retval;
+}
+
+static __inline__ int __generic_coldfire_test_and_clear_bit(int nr,
+	volatile void *vaddr)
+{
+	char retval;
+
+	__asm__ __volatile__ ("bclr %2,%1; sne %0"
+	     : "=d" (retval), "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
+	     : "d" (nr & 7)
+	     : "memory");
+
+	return retval;
+}
+#define __test_and_clear_bit(nr, vaddr) test_and_clear_bit(nr, vaddr)
+
+/*
+ * clear_bit() doesn't provide any barrier for the compiler.
+ */
+#define smp_mb__before_clear_bit()	barrier()
+#define smp_mb__after_clear_bit()	barrier()
+
+#define clear_bit(nr,vaddr)			\
+  (__builtin_constant_p(nr) ?			\
+   __constant_coldfire_clear_bit(nr, vaddr) :	\
+   __generic_coldfire_clear_bit(nr, vaddr))
+
+static __inline__ void __constant_coldfire_clear_bit(int nr,
+	volatile void *vaddr)
+{
+	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
+	__asm__ __volatile__ ("bclr %1,(%3)"
+	     : "=m" (*p) : "id" (nr & 7), "m" (*p), "a" (p));
+}
+
+static __inline__ void __generic_coldfire_clear_bit(int nr,
+	volatile void *vaddr)
+{
+	__asm__ __volatile__ ("bclr %1,%0"
+	     : "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
+	     : "d" (nr)
+	     : "memory");
+}
+#define __clear_bit(nr, vaddr) clear_bit(nr, vaddr)
+
+#define test_and_change_bit(nr, vaddr)			\
+  (__builtin_constant_p(nr) ?				\
+   __constant_coldfire_test_and_change_bit(nr, vaddr) :	\
+   __generic_coldfire_test_and_change_bit(nr, vaddr))
+
+static __inline__ int __constant_coldfire_test_and_change_bit(int nr,
+	volatile void *vaddr)
+{
+	char retval;
+	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
+
+	__asm__ __volatile__ ("bchg %2,(%4); sne %0"
+	     : "=d" (retval), "=m" (*p)
+	     : "id" (nr & 7), "m" (*p), "a" (p));
+
+	return retval;
+}
+
+static __inline__ int __generic_coldfire_test_and_change_bit(int nr,
+	volatile void *vaddr)
+{
+	char retval;
+
+	__asm__ __volatile__ ("bchg %2,%1; sne %0"
+	     : "=d" (retval), "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
+	     : "id" (nr)
+	     : "memory");
+
+	return retval;
+}
+#define __test_and_change_bit(nr, vaddr) test_and_change_bit(nr, vaddr)
+#define __change_bit(nr, vaddr) change_bit(nr, vaddr)
+
+#define change_bit(nr,vaddr)			\
+  (__builtin_constant_p(nr) ?			\
+   __constant_coldfire_change_bit(nr, vaddr) :	\
+   __generic_coldfire_change_bit(nr, vaddr))
+
+static __inline__ void __constant_coldfire_change_bit(int nr,
+	volatile void *vaddr)
+{
+	volatile char *p = &((volatile char *)vaddr)[(nr^31) >> 3];
+	__asm__ __volatile__ ("bchg %1,(%3)"
+	     : "=m" (*p) : "id" (nr & 7), "m" (*p), "a" (p));
+}
+
+static __inline__ void __generic_coldfire_change_bit(int nr,
+	volatile void *vaddr)
+{
+	__asm__ __volatile__ ("bchg %1,%0"
+	     : "=m" (((volatile char *)vaddr)[(nr^31) >> 3])
+	     : "d" (nr)
+	     : "memory");
+}
+
+static inline int test_bit(int nr, const unsigned long *vaddr)
+{
+	return (vaddr[nr >> 5] & (1UL << (nr & 31))) != 0;
+}
+
+static __inline__ unsigned long ffz(unsigned long word)
+{
+	unsigned long result = 0;
+
+	while (word & 1) {
+		result++;
+		word >>= 1;
+	}
+	return result;
+}
+
+/* find_next_zero_bit() finds the first zero bit in a bit string of length
+ * 'size' bits, starting the search at bit 'offset'.  This is largely based
+ * on Linus's ALPHA routines.
+ */
+static __inline__ unsigned long find_next_zero_bit(void *addr,
+	unsigned long size, unsigned long offset)
+{
+	unsigned long *p = ((unsigned long *) addr) + (offset >> 5);
+	unsigned long result = offset & ~31UL;
+	unsigned long tmp;
+
+	if (offset >= size)
+		return size;
+	size -= result;
+	offset &= 31UL;
+	if (offset) {
+		tmp = *(p++);
+		tmp |= ~0UL >> (32-offset);
+		if (size < 32)
+			goto found_first;
+		if (~tmp)
+			goto found_middle;
+		size -= 32;
+		result += 32;
+	}
+	while (size & ~31UL) {
+		tmp = *(p++);
+		if (~tmp)
+			goto found_middle;
+		result += 32;
+		size -= 32;
+	}
+	if (!size)
+		return result;
+	tmp = *p;
+
+found_first:
+	tmp |= ~0UL >> size;
+found_middle:
+	return result + ffz(tmp);
+}
+
+#define find_first_zero_bit(addr, size) find_next_zero_bit(((void *)addr), \
+		(size), 0)
+
+/* Ported from included/linux/bitops.h  */
+static __inline__ int ffs(int x)
+{
+	int r = 1;
+
+	if (!x)
+		return 0;
+	if (!(x & 0xffff)) {
+		x >>= 16;
+		r += 16;
+	}
+	if (!(x & 0xff)) {
+		x >>= 8;
+		r += 8;
+	}
+	if (!(x & 0xf)) {
+		x >>= 4;
+		r += 4;
+	}
+	if (!(x & 3)) {
+		x >>= 2;
+		r += 2;
+	}
+	if (!(x & 1)) {
+		x >>= 1;
+		r += 1;
+	}
+	return r;
+}
+#define __ffs(x) (ffs(x) - 1)
+
+/* find_next_bit - find the next set bit in a memory region
+ * (from asm-ppc/bitops.h)
+ */
+static __inline__ unsigned long find_next_bit(const unsigned long *addr,
+	unsigned long size, unsigned long offset)
+{
+	unsigned int *p = ((unsigned int *) addr) + (offset >> 5);
+	unsigned int result = offset & ~31UL;
+	unsigned int tmp;
+
+	if (offset >= size)
+		return size;
+	size -= result;
+	offset &= 31UL;
+	if (offset) {
+		tmp = *p++;
+		tmp &= ~0UL << offset;
+		if (size < 32)
+			goto found_first;
+		if (tmp)
+			goto found_middle;
+		size -= 32;
+		result += 32;
+	}
+	while (size >= 32) {
+		tmp = *p++;
+		if (tmp != 0)
+			goto found_middle;
+		result += 32;
+		size -= 32;
+	}
+	if (!size)
+		return result;
+	tmp = *p;
+
+found_first:
+	tmp &= ~0UL >> (32 - size);
+	if (tmp == 0UL)        /* Are any bits set? */
+		return result + size; /* Nope. */
+found_middle:
+	return result + __ffs(tmp);
+}
+
+#define find_first_bit(addr, size) find_next_bit((addr), (size), 0)
+
+#ifdef __KERNEL__
+
+/* Ported from include/linux/bitops.h */
+static  __inline__ int fls(int x)
+{
+	int r = 32;
+
+	if (!x)
+		return 0;
+	if (!(x & 0xffff0000u)) {
+		x <<= 16;
+		r -= 16;
+	}
+	if (!(x & 0xff000000u)) {
+		x <<= 8;
+		r -= 8;
+	}
+	if (!(x & 0xf0000000u)) {
+		x <<= 4;
+		r -= 4;
+	}
+	if (!(x & 0xc0000000u)) {
+		x <<= 2;
+		r -= 2;
+	}
+	if (!(x & 0x80000000u)) {
+		x <<= 1;
+		r -= 1;
+	}
+	return r;
+}
+
+#include <asm-generic/bitops/fls64.h>
+#include <asm-generic/bitops/sched.h>
+#include <asm-generic/bitops/hweight.h>
+#include <asm-generic/bitops/lock.h>
+
+#define minix_find_first_zero_bit(addr, size)	find_next_zero_bit((addr), \
+							(size), 0)
+#define minix_test_and_set_bit(nr, addr)	test_and_set_bit((nr), \
+							(unsigned long *)(addr))
+#define minix_set_bit(nr, addr)			set_bit((nr), \
+							(unsigned long *)(addr))
+#define minix_test_and_clear_bit(nr, addr)	test_and_clear_bit((nr), \
+							(unsigned long *)(addr))
+
+static inline int minix_test_bit(int nr, const volatile unsigned long *vaddr)
+{
+	int 	*a = (int *)vaddr;
+	int	mask;
+
+	a += nr >> 5;
+	mask = 1 << (nr & 0x1f);
+	return ((mask & *a) != 0);
+}
+
+#define ext2_set_bit(nr, addr)			test_and_set_bit((nr) ^ 24, \
+							(unsigned long *)(addr))
+#define ext2_set_bit_atomic(lock, nr, addr)	test_and_set_bit((nr) ^ 24, \
+							(unsigned long *)(addr))
+#define ext2_clear_bit(nr, addr)		test_and_clear_bit((nr) ^ 24, \
+							(unsigned long *)(addr))
+#define ext2_clear_bit_atomic(lock, nr, addr)	test_and_clear_bit((nr) ^ 24, \
+							(unsigned long *)(addr))
+
+static inline int ext2_test_bit(int nr, const void *vaddr)
+{
+	const unsigned char *p = vaddr;
+	return (p[nr >> 3] & (1U << (nr & 7))) != 0;
+}
+
+static inline int ext2_find_first_zero_bit(const void *vaddr, unsigned size)
+{
+	const unsigned long *p = vaddr, *addr = vaddr;
+	int res;
+
+	if (!size)
+		return 0;
+
+	size = (size >> 5) + ((size & 31) > 0);
+	while (*p++ == ~0UL) {
+		if (--size == 0)
+			return (p - addr) << 5;
+	}
+
+	--p;
+	for (res = 0; res < 32; res++)
+		if (!ext2_test_bit (res, p))
+			break;
+	return (p - addr) * 32 + res;
+}
+
+static inline int ext2_find_next_zero_bit(const void *vaddr, unsigned size,
+					  unsigned offset)
+{
+	const unsigned long *addr = vaddr;
+	const unsigned long *p = addr + (offset >> 5);
+	int bit = offset & 31UL, res;
+
+	if (offset >= size)
+		return size;
+
+	if (bit) {
+		/* Look for zero in first longword */
+		for (res = bit; res < 32; res++)
+			if (!ext2_test_bit (res, p))
+				return (p - addr) * 32 + res;
+		p++;
+	}
+	/* No zero yet, search remaining full bytes for a zero */
+	res = ext2_find_first_zero_bit(p, size - 32 * (p - addr));
+	return (p - addr) * 32 + res;
+}
+
+#endif /* KERNEL */
+
+#endif /* __CF_BITOPS__ */
diff --git a/include/asm-m68k/cf_io.h b/include/asm-m68k/cf_io.h
new file mode 100644
index 0000000..ab242b7
--- /dev/null
+++ b/include/asm-m68k/cf_io.h
@@ -0,0 +1,175 @@
+/*
+ * linux/include/asm-m68k/cf_io.h
+ *
+ * 9/30/08 JKM - Separated Coldfire pieces out from m68k.
+ */
+
+#ifndef __CF_IO__
+#define __CF_IO__
+
+#ifdef __KERNEL__
+
+#include <linux/compiler.h>
+#include <asm/raw_io.h>
+#include <asm/virtconvert.h>
+
+#include <asm-generic/iomap.h>
+
+/*
+ * These should be valid on any ioremap()ed region
+ */
+#define readb(addr)      in_8(addr)
+#define writeb(val,addr) out_8((addr),(val))
+#define readw(addr)      in_le16(addr)
+#define writew(val,addr) out_le16((addr),(val))
+#define readl(addr)      in_le32(addr)
+#define writel(val,addr) out_le32((addr),(val))
+
+#define readb_relaxed(addr) readb(addr)
+#define readw_relaxed(addr) readw(addr)
+#define readl_relaxed(addr) readl(addr)
+
+#ifdef CONFIG_PCI
+#define inb_p   inb
+#define inw_p   inw
+#define inl_p   inl
+#define outb_p  outb
+#define outw_p  outw
+#define outl_p  outl
+
+#ifndef CONFIG_COLDFIRE
+#define inb(port)      in_8(port)
+#define outb(val,port) out_8((port),(val))
+#define inw(port)      in_le16(port)
+#define outw(val,port) out_le16((port),(val))
+#define inl(port)      in_le32(port)
+#define outl(val,port) out_le32((port),(val))
+#define insb(port, buf, nr)	\
+		raw_insb((u8 *)(port), (u8 *)(buf), (nr))
+#define outsb(port, buf, nr)	\
+		raw_outsb((u8 *)(port), (u8 *)(buf), (nr))
+#define insw(port, buf, nr)	\
+		raw_insw_swapw((u16 *)(port), (u16 *)(buf), (nr))
+#define outsw(port, buf, nr)	\
+		raw_outsw_swapw((u16 *)(port), (u16 *)(buf), (nr))
+#define insl(port, buf, nr)	\
+		raw_insw_swapw((u16 *)(port), (u16 *)(buf), (nr)<<1)
+#define outsl(port, buf, nr)	\
+		raw_outsw_swapw((u16 *)(port), (u16 *)(buf), (nr)<<1)
+#else
+#define inb(port)      pci_inb(port)
+#define outb(val,port) pci_outb((val),(port))
+#define inw(port)      pci_inw(port)
+#define outw(val,port) pci_outw((val),(port))
+#define insb(a,b,c)  pci_insb((volatile unsigned char*)a,(unsigned char*)b,c)
+#define insw(a,b,c)  pci_insw((volatile unsigned short*)a,(const unsigned short*)b,c)
+#define insl(a,b,c)  pci_insl((volatile unsigned long*)a,(const unsigned long*)b,c)
+#define outsb(a,b,c) pci_outsb((volatile unsigned char*)a,(const unsigned char*)b,c)
+#define outsw(a,b,c) pci_outsw((volatile unsigned short*)a,(const unsigned short*)b,c)
+#define outsl(a,b,c) pci_outsl((volatile unsigned long*)a,(const unsigned long*)b,c)
+#define inl(port)        pci_inl(port)
+#define outl(val,port)   pci_outl((val),(port))
+#endif
+
+#else
+/* no pci */
+
+#define inb(port)      in_8(port)
+#define outb(val,port) out_8((port),(val))
+#define inw(port)      in_le16(port)
+#define outw(val,port) out_le16((port),(val))
+#define inl(port)      in_le32(port)
+#define outl(val,port) out_le32((port),(val))
+#define insb(port, buf, nr)	\
+		raw_insb((u8 *)(port), (u8 *)(buf), (nr))
+#define outsb(port, buf, nr)	\
+		raw_outsb((u8 *)(port), (u8 *)(buf), (nr))
+#define insw(port, buf, nr)	\
+		raw_insw_swapw((u16 *)(port), (u16 *)(buf), (nr))
+#define outsw(port, buf, nr)	\
+		raw_outsw_swapw((u16 *)(port), (u16 *)(buf), (nr))
+#define insl(port, buf, nr)	\
+		raw_insw_swapw((u16 *)(port), (u16 *)(buf), (nr)<<1)
+#define outsl(port, buf, nr)	\
+		raw_outsw_swapw((u16 *)(port), (u16 *)(buf), (nr)<<1)
+
+#endif /* CONFIG_PCI */
+
+#define mmiowb()
+
+static inline void __iomem *ioremap(unsigned long physaddr, unsigned long size)
+{
+	return __ioremap(physaddr, size, IOMAP_NOCACHE_SER);
+}
+static inline void __iomem *ioremap_nocache(unsigned long physaddr, unsigned long size)
+{
+	return __ioremap(physaddr, size, IOMAP_NOCACHE_SER);
+}
+static inline void __iomem *ioremap_writethrough(unsigned long physaddr,
+					 unsigned long size)
+{
+	return __ioremap(physaddr, size, IOMAP_WRITETHROUGH);
+}
+static inline void __iomem *ioremap_fullcache(unsigned long physaddr,
+				      unsigned long size)
+{
+	return __ioremap(physaddr, size, IOMAP_FULL_CACHING);
+}
+
+static inline void memset_io(volatile void __iomem *addr, unsigned char val, int count)
+{
+	__builtin_memset((void __force *) addr, val, count);
+}
+static inline void memcpy_fromio(void *dst, const volatile void __iomem *src, int count)
+{
+	__builtin_memcpy(dst, (void __force *) src, count);
+}
+static inline void memcpy_toio(volatile void __iomem *dst, const void *src, int count)
+{
+	__builtin_memcpy((void __force *) dst, src, count);
+}
+
+#define IO_SPACE_LIMIT 0xffffffff
+
+#endif /* __KERNEL__ */
+
+#define __ARCH_HAS_NO_PAGE_ZERO_MAPPED		1
+
+/*
+ * Convert a physical pointer to a virtual kernel pointer for /dev/mem
+ * access
+ */
+#define xlate_dev_mem_ptr(p)	__va(p)
+
+/*
+ * Convert a virtual cached pointer to an uncached pointer
+ */
+#define xlate_dev_kmem_ptr(p)	p
+
+#define __raw_readb(addr) \
+    ({ unsigned char __v = (*(volatile unsigned char *) (addr)); __v; })
+#define __raw_readw(addr) \
+    ({ unsigned short __v = (*(volatile unsigned short *) (addr)); __v; })
+#define __raw_readl(addr) \
+    ({ unsigned long __v = (*(volatile unsigned long *) (addr)); __v; })
+#define __raw_writeb(b,addr) (void)((*(volatile unsigned char *) (addr)) = (b))
+#define __raw_writew(b,addr) (void)((*(volatile unsigned short *) (addr)) = (b))
+#define __raw_writel(b,addr) (void)((*(volatile unsigned int *) (addr)) = (b))
+
+#define memset_io(a, b, c) memset((void *)(a), (b), (c))
+#define memcpy_fromio(a, b, c) memcpy((a), (void *)(b), (c))
+#define memcpy_toio(a, b, c) memcpy((void *)(a), (b), (c))
+
+#if !defined(readb)
+#define readb(addr) \
+    ({ unsigned char __v = (*(volatile unsigned char *) (addr)); __v; })
+#define readw(addr) \
+    ({ unsigned short __v = (*(volatile unsigned short *) (addr)); __v; })
+#define readl(addr) \
+    ({ unsigned int __v = (*(volatile unsigned int *) (addr)); __v; })
+#define writeb(b, addr) (void)((*(volatile unsigned char *) (addr)) = (b))
+#define writew(b, addr) (void)((*(volatile unsigned short *) (addr)) = (b))
+#define writel(b, addr) (void)((*(volatile unsigned int *) (addr)) = (b))
+#endif /* readb */
+
+#endif /* _IO_H */
diff --git a/include/asm-m68k/cf_raw_io.h b/include/asm-m68k/cf_raw_io.h
new file mode 100644
index 0000000..6200173
--- /dev/null
+++ b/include/asm-m68k/cf_raw_io.h
@@ -0,0 +1,167 @@
+/*
+ * linux/include/asm-m68k/cf_raw_io.h
+ *
+ * 09/30/08 JKM: split Coldfire pieces into separate file
+ */
+#ifndef __CF_RAW_IO__
+#define __CF_RAW_IO__
+
+#ifdef __KERNEL__
+
+#include <asm/types.h>
+
+/* Values for nocacheflag and cmode */
+#define IOMAP_FULL_CACHING		0
+#define IOMAP_NOCACHE_SER		1
+#define IOMAP_NOCACHE_NONSER		2
+#define IOMAP_WRITETHROUGH		3
+
+extern void iounmap(void __iomem *addr);
+
+extern void __iomem *__ioremap(unsigned long physaddr, unsigned long size,
+		       int cacheflag);
+extern void __iounmap(void *addr, unsigned long size);
+
+
+/* ++roman: The assignments to temp. vars avoid that gcc sometimes generates
+ * two accesses to memory, which may be undesirable for some devices.
+ */
+#define in_8(addr) \
+    ({ u8 __v = (*(__force volatile u8 *) (addr)); __v; })
+#define in_be16(addr) \
+    ({ u16 __v = (*(__force volatile u16 *) (addr)); __v; })
+#define in_be32(addr) \
+    ({ u32 __v = (*(__force volatile u32 *) (addr)); __v; })
+#define in_le16(addr) \
+    ({ u16 __v = le16_to_cpu(*(__force volatile __le16 *) (addr)); __v; })
+#define in_le32(addr) \
+    ({ u32 __v = le32_to_cpu(*(__force volatile __le32 *) (addr)); __v; })
+
+#define out_8(addr,b) (void)((*(__force volatile u8 *) (addr)) = (b))
+#define out_be16(addr,w) (void)((*(__force volatile u16 *) (addr)) = (w))
+#define out_be32(addr,l) (void)((*(__force volatile u32 *) (addr)) = (l))
+#define out_le16(addr,w) (void)((*(__force volatile __le16 *) (addr)) = cpu_to_le16(w))
+#define out_le32(addr,l) (void)((*(__force volatile __le32 *) (addr)) = cpu_to_le32(l))
+
+
+#ifdef CONFIG_PCI
+/* pci */
+unsigned char  pci_inb(long addr);
+unsigned short pci_inw(long addr);
+unsigned long  pci_inl(long addr);
+
+void pci_outb(unsigned char  val, long addr);
+void pci_outw(unsigned short val, long addr);
+void pci_outl(unsigned long  val, long addr);
+
+void pci_insb(volatile unsigned char* addr, unsigned char* buf, int len);
+void pci_insw(volatile unsigned short* addr, unsigned short* buf, int len);
+void pci_insl(volatile unsigned long* addr, unsigned long* buf, int len);
+
+void pci_outsb(volatile unsigned char* addr, const unsigned char* buf, int len);
+void pci_outsw(volatile unsigned short* addr, const unsigned short* buf, int len);
+void pci_outsl(volatile unsigned long* addr, const unsigned long* buf, int len);
+
+unsigned short pci_raw_inw(long addr);
+unsigned long  pci_raw_inl(long addr);
+void pci_raw_outw(unsigned short val, long addr);
+void pci_raw_outl(unsigned long  val, long addr);
+
+#define raw_inb(port) pci_inb((long)((volatile unsigned char *)(port)))
+#define raw_inw(port) pci_raw_inw((long)((volatile unsigned short *)(port)))
+#define raw_inl(port) pci_raw_inl((long)((volatile unsigned long *)(port)))
+
+#define raw_outb(val,port) pci_outb((val),(long)((volatile unsigned char *)(port)))
+#define raw_outw(val,port) pci_raw_outw((val),(long)((volatile unsigned short *)(port)))
+#define raw_outl(val,port) pci_raw_outl((val),(long)((volatile unsigned long *)(port)))
+
+#define swap_inw(port) pci_inw((long)((volatile unsigned short *)(port)))
+#define swap_outw(val,port) pci_outw((val),(long)((volatile unsigned short *)(port)))
+
+#else
+/* non-pci */
+#define raw_inb in_8
+#define raw_inw in_be16
+#define raw_inl in_be32
+
+#define raw_outb(val,port) out_8((port),(val))
+#define raw_outw(val,port) out_be16((port),(val))
+#define raw_outl(val,port) out_be32((port),(val))
+
+#define swap_inw(port) in_le16((port))
+#define swap_outw(val,port) out_le16((port),(val))
+#endif
+
+static inline void raw_insb(volatile u8 __iomem *port, u8 *buf, unsigned int len)
+{
+	unsigned int i;
+
+        for (i = 0; i < len; i++)
+		*buf++ = in_8(port);
+}
+
+static inline void raw_outsb(volatile u8 __iomem *port, const u8 *buf,
+			     unsigned int len)
+{
+	unsigned int i;
+
+        for (i = 0; i < len; i++)
+		out_8(port, *buf++);
+}
+
+static inline void raw_insw(volatile u16 *port, u16 *buf, unsigned int nr)
+{
+	unsigned int i;
+
+	for (i = 0; i < nr; i++)
+		*buf++ = raw_inw(port);
+}
+
+static inline void raw_outsw(volatile u16 *port, const u16 *buf,
+	unsigned int nr)
+{
+	unsigned int i;
+
+	for (i = 0; i < nr; i++, buf++)
+		raw_outw(*buf, port);
+}
+
+static inline void raw_insl(volatile u32 *port, u32 *buf, unsigned int nr)
+{
+	unsigned int i;
+
+	for (i = 0; i < nr; i++)
+		*buf++ = raw_inl(port);
+}
+
+static inline void raw_outsl(volatile u32 *port, const u32 *buf,
+	unsigned int nr)
+{
+	unsigned int i;
+
+	for (i = 0; i < nr; i++, buf++)
+		raw_outl(*buf, port);
+}
+
+static inline void raw_insw_swapw(volatile u16 *port, u16 *buf,
+				  unsigned int nr)
+{
+	unsigned int i;
+
+	for (i = 0; i < nr; i++)
+		*buf++ = in_le16(port);
+
+}
+
+static inline void raw_outsw_swapw(volatile u16 __iomem *port, const u16 *buf,
+				   unsigned int nr)
+{
+	unsigned int i;
+
+	for (i = 0; i < nr; i++, buf++)
+		out_le16(port, *buf);
+}
+
+#endif /* __KERNEL__ */
+
+#endif /* __CF_RAW_IO__ */
diff --git a/include/asm-m68k/cf_virtconvert.h b/include/asm-m68k/cf_virtconvert.h
new file mode 100644
index 0000000..19df86d
--- /dev/null
+++ b/include/asm-m68k/cf_virtconvert.h
@@ -0,0 +1,55 @@
+#ifndef __CF_VIRTCONVERT__
+#define __CF_VIRTCONVERT__
+
+/*
+ * Macros used for converting between virtual and physical mappings.
+ *
+ * Coldfire Specific
+ */
+
+#ifdef __KERNEL__
+
+#include <linux/compiler.h>
+#include <linux/mmzone.h>
+#include <asm/setup.h>
+#include <asm/page.h>
+
+/*
+ * Change virtual addresses to physical addresses and vv.
+ */
+static inline unsigned long virt_to_phys(void *address)
+{
+	return __pa(address);
+}
+
+static inline void *phys_to_virt(unsigned long address)
+{
+	return __va(address);
+}
+
+/* Permanent address of a page. */
+#ifdef CONFIG_SINGLE_MEMORY_CHUNK
+#define page_to_phys(page) \
+	__pa(PAGE_OFFSET + (((page) - pg_data_map[0].node_mem_map) << PAGE_SHIFT))
+#else
+#define page_to_phys(_page) ({						\
+	struct page *__page = _page;					\
+	struct pglist_data *pgdat;					\
+	pgdat = pg_data_table[page_to_nid(__page)];			\
+	page_to_pfn(__page) << PAGE_SHIFT;				\
+})
+#endif
+
+/*
+ * IO bus memory addresses are 1:1 with the physical address,
+ */
+#ifdef CONFIG_PCI
+#define virt_to_bus(a) (a + PCI_DMA_BASE)
+#define bus_to_virt(a) (a - PCI_DMA_BASE)
+#else
+#define virt_to_bus(a) (a)
+#define bus_to_virt(a) (a)
+#endif
+
+#endif	/* __KERNEL__ */
+#endif	/* __CF_VIRTCONVERT__ */
diff --git a/include/asm-m68k/io.h b/include/asm-m68k/io.h
index 53662b8..004c73e 100644
--- a/include/asm-m68k/io.h
+++ b/include/asm-m68k/io.h
@@ -21,6 +21,10 @@
 #ifndef _IO_H
 #define _IO_H
 
+#ifdef CONFIG_COLDFIRE
+#include <asm/cf_io.h>
+#else
+
 #ifdef __KERNEL__
 
 #include <linux/compiler.h>
@@ -204,21 +208,10 @@ static inline u16 __iomem *isa_mtw(unsigned long addr)
 #define isa_outw(val,port) (ISA_SEX ? out_be16(isa_itw(port),(val)) : out_le16(isa_itw(port),(val)))
 #define isa_outl(val,port) (ISA_SEX ? out_be32(isa_itl(port),(val)) : out_le32(isa_itl(port),(val)))
 
-#ifndef CONFIG_COLDFIRE
 #define isa_readb(p)       in_8(isa_mtb(p))
 #define isa_readw(p)       (ISA_SEX ? in_be16(isa_mtw(p)) : in_le16(isa_mtw(p)))
 #define isa_writeb(val,p)  out_8(isa_mtb(p),(val))
 #define isa_writew(val,p)  (ISA_SEX ? out_be16(isa_mtw(p),(val)) : out_le16(isa_mtw(p),(val)))
-#else
-#define isa_readb(p)       in_8(isa_mtb((unsigned long)(p)))
-#define isa_readw(p)       \
-	(ISA_SEX ? in_be16(isa_mtw((unsigned long)(p)))	\
-		 : in_le16(isa_mtw((unsigned long)(p))))
-#define isa_writeb(val,p)  out_8(isa_mtb((unsigned long)(p)),(val))
-#define isa_writew(val,p)  \
-	(ISA_SEX ? out_be16(isa_mtw((unsigned long)(p)),(val))	\
-		 : out_le16(isa_mtw((unsigned long)(p)),(val)))
-#endif
 static inline void isa_delay(void)
 {
   switch(ISA_TYPE)
@@ -291,29 +284,6 @@ static inline void isa_delay(void)
 #endif /* CONFIG_ISA */
 
 #if defined(CONFIG_PCI)
-#ifdef CONFIG_COLDFIRE
-#define inb_p   inb
-#define inw_p   inw
-#define inl_p   inl
-#define outb_p  outb
-#define outw_p  outw
-#define outl_p  outl
-
-unsigned char  pci_inb(long addr);
-unsigned short pci_inw(long addr);
-unsigned long  pci_inl(long addr);
-void pci_outb(unsigned char  val, long addr);
-void pci_outw(unsigned short val, long addr);
-void pci_outl(unsigned long  val, long addr);
-
-void pci_insb(volatile unsigned char* addr, unsigned char* buf, int len);
-void pci_insw(volatile unsigned short* addr, unsigned short* buf, int len);
-void pci_insl(volatile unsigned long* addr, unsigned long* buf, int len);
-
-void pci_outsb(volatile unsigned char* addr, const unsigned char* buf, int len);
-void pci_outsw(volatile unsigned short* addr, const unsigned short* buf, int len);
-void pci_outsl(volatile unsigned long* addr, const unsigned long* buf, int len);
-#endif
 
 #define readl(addr)      in_le32(addr)
 #define writel(val,addr) out_le32((addr),(val))
@@ -329,7 +299,6 @@ void pci_outsl(volatile unsigned long* addr, const unsigned long* buf, int len);
 #define readl_relaxed(addr) readl(addr)
 
 #ifndef CONFIG_ISA
-#ifndef CONFIG_COLDFIRE
 #define inb(port)      in_8(port)
 #define outb(val,port) out_8((port),(val))
 #define inw(port)      in_le16(port)
@@ -348,29 +317,13 @@ void pci_outsl(volatile unsigned long* addr, const unsigned long* buf, int len);
 		raw_insw_swapw((u16 *)(port), (u16 *)(buf), (nr)<<1)
 #define outsl(port, buf, nr)	\
 		raw_outsw_swapw((u16 *)(port), (u16 *)(buf), (nr)<<1)
-#else
-#define inb(port)      pci_inb(port)
-#define outb(val,port) pci_outb((val),(port))
-#define inw(port)      pci_inw(port)
-#define outw(val,port) pci_outw((val),(port))
-#define insb(a,b,c)  pci_insb((volatile unsigned char*)a,(unsigned char*)b,c)
-#define insw(a,b,c)  pci_insw((volatile unsigned short*)a,(const unsigned short*)b,c)
-#define insl(a,b,c)  pci_insl((volatile unsigned long*)a,(const unsigned long*)b,c)
-#define outsb(a,b,c) pci_outsb((volatile unsigned char*)a,(const unsigned char*)b,c)
-#define outsw(a,b,c) pci_outsw((volatile unsigned short*)a,(const unsigned short*)b,c)
-#define outsl(a,b,c) pci_outsl((volatile unsigned long*)a,(const unsigned long*)b,c)
-#define inl(port)        pci_inl(port)
-#define outl(val,port)   pci_outl((val),(port))
-#endif
 
-#ifndef CONFIG_COLDFIRE
 #define __raw_readb readb
 #define __raw_readw readw
 #define __raw_readl readl
 #define __raw_writeb writeb
 #define __raw_writew writew
 #define __raw_writel writel
-#endif
 
 #else
 /*
@@ -464,8 +417,6 @@ static inline void memcpy_toio(volatile void __iomem *dst, const void *src, int
 
 #if defined(CONFIG_SUN3)
 #define IO_SPACE_LIMIT 0x0fffffff
-#elif defined(CONFIG_COLDFIRE)
-#define IO_SPACE_LIMIT 0xffffffff
 #else
 #define IO_SPACE_LIMIT 0xffff
 #endif
@@ -485,31 +436,5 @@ static inline void memcpy_toio(volatile void __iomem *dst, const void *src, int
  */
 #define xlate_dev_kmem_ptr(p)	p
 
-#ifdef CONFIG_COLDFIRE
-#define __raw_readb(addr) \
-    ({ unsigned char __v = (*(volatile unsigned char *) (addr)); __v; })
-#define __raw_readw(addr) \
-    ({ unsigned short __v = (*(volatile unsigned short *) (addr)); __v; })
-#define __raw_readl(addr) \
-    ({ unsigned long __v = (*(volatile unsigned long *) (addr)); __v; })
-#define __raw_writeb(b,addr) (void)((*(volatile unsigned char *) (addr)) = (b))
-#define __raw_writew(b,addr) (void)((*(volatile unsigned short *) (addr)) = (b))
-#define __raw_writel(b,addr) (void)((*(volatile unsigned int *) (addr)) = (b))
-
-#define memset_io(a, b, c) memset((void *)(a), (b), (c))
-#define memcpy_fromio(a, b, c) memcpy((a), (void *)(b), (c))
-#define memcpy_toio(a, b, c) memcpy((void *)(a), (b), (c))
-#if !defined(readb)
-#define readb(addr) \
-    ({ unsigned char __v = (*(volatile unsigned char *) (addr)); __v; })
-#define readw(addr) \
-    ({ unsigned short __v = (*(volatile unsigned short *) (addr)); __v; })
-#define readl(addr) \
-    ({ unsigned int __v = (*(volatile unsigned int *) (addr)); __v; })
-#define writeb(b, addr) (void)((*(volatile unsigned char *) (addr)) = (b))
-#define writew(b, addr) (void)((*(volatile unsigned short *) (addr)) = (b))
-#define writel(b, addr) (void)((*(volatile unsigned int *) (addr)) = (b))
-#endif /* readb */
 #endif /* CONFIG_COLDFIRE */
-
 #endif /* _IO_H */
diff --git a/include/asm-m68k/raw_io.h b/include/asm-m68k/raw_io.h
index ee8a308..c018faa 100644
--- a/include/asm-m68k/raw_io.h
+++ b/include/asm-m68k/raw_io.h
@@ -8,6 +8,10 @@
 #ifndef _RAW_IO_H
 #define _RAW_IO_H
 
+#ifdef CONFIG_COLDFIRE
+#include <asm/cf_raw_io.h>
+#else
+
 #ifdef __KERNEL__
 
 #include <asm/types.h>
@@ -46,50 +50,23 @@ extern void __iounmap(void *addr, unsigned long size);
 #define out_le16(addr,w) (void)((*(__force volatile __le16 *) (addr)) = cpu_to_le16(w))
 #define out_le32(addr,l) (void)((*(__force volatile __le32 *) (addr)) = cpu_to_le32(l))
 
-#if (defined CONFIG_COLDFIRE) && (defined CONFIG_PCI)
-unsigned char  pci_inb(long addr);
-unsigned short pci_inw(long addr);
-unsigned long  pci_inl(long addr);
-void pci_outb(unsigned char  val, long addr);
-void pci_outw(unsigned short val, long addr);
-void pci_outl(unsigned long  val, long addr);
-unsigned short pci_raw_inw(long addr);
-unsigned long  pci_raw_inl(long addr);
-void pci_raw_outw(unsigned short val, long addr);
-void pci_raw_outl(unsigned long  val, long addr);
-#define raw_inb(port) pci_inb((long)((volatile unsigned char *)(port)))
-#define raw_inw(port) pci_raw_inw((long)((volatile unsigned short *)(port)))
-#define raw_inl(port) pci_raw_inl((long)((volatile unsigned long *)(port)))
-
-#define raw_outb(val,port) pci_outb((val),(long)((volatile unsigned char *)(port)))
-#define raw_outw(val,port) pci_raw_outw((val),(long)((volatile unsigned short *)(port)))
-#define raw_outl(val,port) pci_raw_outl((val),(long)((volatile unsigned long *)(port)))
-
-#define swap_inw(port) pci_inw((long)((volatile unsigned short *)(port)))
-#define swap_outw(val,port) pci_outw((val),(long)((volatile unsigned short *)(port)))
-
-#else
 #define raw_inb in_8
 #define raw_inw in_be16
 #define raw_inl in_be32
-#ifndef CONFIG_COLDFIRE
 #define __raw_readb in_8
 #define __raw_readw in_be16
 #define __raw_readl in_be32
-#endif
 
 #define raw_outb(val,port) out_8((port),(val))
 #define raw_outw(val,port) out_be16((port),(val))
 #define raw_outl(val,port) out_be32((port),(val))
-#ifndef CONFIG_COLDFIRE
 #define __raw_writeb(val,addr) out_8((addr),(val))
 #define __raw_writew(val,addr) out_be16((addr),(val))
 #define __raw_writel(val,addr) out_be32((addr),(val))
-#endif
 
 #define swap_inw(port) in_le16((port))
 #define swap_outw(val,port) out_le16((port),(val))
-#endif
+
 static inline void raw_insb(volatile u8 __iomem *port, u8 *buf, unsigned int len)
 {
 	unsigned int i;
@@ -107,7 +84,6 @@ static inline void raw_outsb(volatile u8 __iomem *port, const u8 *buf,
 		out_8(port, *buf++);
 }
 
-#ifndef CONFIG_COLDFIRE
 static inline void raw_insw(volatile u16 __iomem *port, u16 *buf, unsigned int nr)
 {
 	unsigned int tmp;
@@ -373,63 +349,8 @@ static inline void raw_outsw_swapw(volatile u16 __iomem *port, const u16 *buf,
 		: "d0", "a0", "a1", "d6");
 }
 
-
-#else /*CONFIG_COLDFIRE */
-
-static inline void raw_insw(volatile u16 *port, u16 *buf, unsigned int nr)
-{
-	unsigned int i;
-
-	for (i = 0; i < nr; i++)
-		*buf++ = raw_inw(port);
-}
-
-static inline void raw_outsw(volatile u16 *port, const u16 *buf,
-	unsigned int nr)
-{
-	unsigned int i;
-
-	for (i = 0; i < nr; i++, buf++)
-		raw_outw(*buf, port);
-}
-
-static inline void raw_insl(volatile u32 *port, u32 *buf, unsigned int nr)
-{
-	unsigned int i;
-
-	for (i = 0; i < nr; i++)
-		*buf++ = raw_inl(port);
-}
-
-static inline void raw_outsl(volatile u32 *port, const u32 *buf,
-	unsigned int nr)
-{
-	unsigned int i;
-
-	for (i = 0; i < nr; i++, buf++)
-		raw_outl(*buf, port);
-}
-
-static inline void raw_insw_swapw(volatile u16 *port, u16 *buf,
-				  unsigned int nr)
-{
-	unsigned int i;
-
-	for (i = 0; i < nr; i++)
-		*buf++ = in_le16(port);
-
-}
-
-static inline void raw_outsw_swapw(volatile u16 __iomem *port, const u16 *buf,
-				   unsigned int nr)
-{
-	unsigned int i;
-
-	for (i = 0; i < nr; i++, buf++)
-		out_le16(port, *buf);
-}
-#endif /*CONFIG_COLDFIRE */
-
 #endif /* __KERNEL__ */
 
+#endif /* CONFIG_COLDFIRE */
+
 #endif /* _RAW_IO_H */
diff --git a/include/asm-m68k/virtconvert.h b/include/asm-m68k/virtconvert.h
index 13c5752..b0a6081 100644
--- a/include/asm-m68k/virtconvert.h
+++ b/include/asm-m68k/virtconvert.h
@@ -1,6 +1,10 @@
 #ifndef __VIRT_CONVERT__
 #define __VIRT_CONVERT__
 
+#ifdef CONFIG_COLDFIRE
+#include <asm/cf_virtconvert.h>
+#else
+
 /*
  * Macros used for converting between virtual and physical mappings.
  */
@@ -46,19 +50,10 @@ static inline void *phys_to_virt(unsigned long address)
 #define virt_to_bus(a) (virt_to_phys(a) + (MACH_IS_HADES ? 0x80000000 : 0))
 #define bus_to_virt(a) (phys_to_virt((a) - (MACH_IS_HADES ? 0x80000000 : 0)))
 #else
-#ifdef CONFIG_COLDFIRE
-#ifdef CONFIG_PCI
-#define virt_to_bus(a) (a - PAGE_OFFSET + PCI_DMA_BASE)
-#define bus_to_virt(a) (a - PCI_DMA_BASE + PAGE_OFFSET)
-#else
-#define virt_to_bus(a) (a - PAGE_OFFSET)
-#define bus_to_virt(a) (a + PAGE_OFFSET)
-#endif
-#else /* CONFIG_COLDFIRE */
 #define virt_to_bus virt_to_phys
 #define bus_to_virt phys_to_virt
 #endif
-#endif
 
 #endif
 #endif
+#endif
-- 
1.6.0.1

